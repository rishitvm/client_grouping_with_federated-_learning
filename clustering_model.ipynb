{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('CICIDS_15.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'Dst Port',\n",
    "    'Timestamp',\n",
    "    'Fwd PSH Flags',\n",
    "    'Bwd PSH Flags',\n",
    "    'Fwd URG Flags',\n",
    "    'Bwd URG Flags',\n",
    "    'Flow Byts/s', \n",
    "    'Flow Pkts/s',\n",
    "    'Protocol'\n",
    "]\n",
    "df.drop(columns=columns_to_drop,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "print(df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "string_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in string_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "df_resampled['Label'] = y_resampled\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(df_resampled['Label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df_shuffled = shuffle(df_resampled)\n",
    "split_size = len(df_shuffled) // 8\n",
    "\n",
    "df_part1 = df_shuffled[:split_size]\n",
    "df_part2 = df_shuffled[split_size:2*split_size]\n",
    "df_part3 = df_shuffled[2*split_size:3*split_size]\n",
    "df_part4 = df_shuffled[3*split_size:4*split_size]\n",
    "df_part5 = df_shuffled[4*split_size:5*split_size]\n",
    "df_part6 = df_shuffled[5*split_size:6*split_size]\n",
    "df_part7 = df_shuffled[6*split_size:7*split_size]\n",
    "df_part8 = df_shuffled[7*split_size:]\n",
    "\n",
    "file_names = [\"client1.csv\", \"client2.csv\", \"client3.csv\", \"client4.csv\",\"client5.csv\", \"client6.csv\", \"client7.csv\", \"client8.csv\"]\n",
    "\n",
    "for part, file_name in zip([df_part1, df_part2, df_part3, df_part4, df_part5, df_part6, df_part7, df_part8], file_names):\n",
    "    part.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "client1_df = pd.read_csv(\"client1.csv\")\n",
    "num_parts = 20\n",
    "split_size = len(client1_df) // num_parts\n",
    "losses = []\n",
    "for i in range(num_parts):\n",
    "    start_idx = i * split_size\n",
    "    end_idx = start_idx + split_size\n",
    "    part_df = client1_df.iloc[start_idx:end_idx]\n",
    "    X = part_df.drop(columns=['Label'])\n",
    "    y = part_df['Label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    loss = mean_squared_error(y_test, y_pred)\n",
    "    losses.append(loss)\n",
    "\n",
    "for i, loss in enumerate(losses, start=1):\n",
    "    print(f\"Loss for model {i}: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client1_df = pd.read_csv(\"client1.csv\")\n",
    "client2_df = pd.read_csv(\"client2.csv\")\n",
    "client3_df = pd.read_csv(\"client3.csv\")\n",
    "client4_df = pd.read_csv(\"client4.csv\")\n",
    "client5_df = pd.read_csv(\"client5.csv\")\n",
    "client6_df = pd.read_csv(\"client6.csv\")\n",
    "client7_df = pd.read_csv(\"client7.csv\")\n",
    "client8_df = pd.read_csv(\"client8.csv\")\n",
    "\n",
    "loss_of_all_models = []\n",
    "client_dfs = [client1_df, client2_df, client3_df,client4_df, client5_df, client6_df,client7_df, client8_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_feedback(X_train, y_train, X_test, y_test, avg_weights):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.set_weights(avg_weights)\n",
    "    model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, verbose=0)\n",
    "    weights = model.get_weights()\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    #accuracy  =accuracy_score(y_test,y_pred)\n",
    "    loss = mean_squared_error(y_test, y_pred)\n",
    "    return weights, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_loss = float('inf')\n",
    "threshold = 2.0\n",
    "d = 1\n",
    "consecutive_rounds = 0\n",
    "stabilization_rounds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server(all_weights, all_losses, num_clusters):\n",
    "    num_clients = len(all_weights)\n",
    "    max_num_layers = max(len(weights) for weights in all_weights)\n",
    "    common_layer_weights = [[] for _ in range(max_num_layers)]\n",
    "\n",
    "    for layer_idx in range(max_num_layers):\n",
    "        layer_weights = []\n",
    "        for client_weights in all_weights:\n",
    "            if layer_idx < len(client_weights):\n",
    "                layer_weights.append(client_weights[layer_idx])\n",
    "        if len(layer_weights) == num_clients:\n",
    "            common_layer_weights[layer_idx] = layer_weights\n",
    "\n",
    "    avg_weights = []\n",
    "    for layer_weights in common_layer_weights:\n",
    "        if layer_weights:\n",
    "            avg_weights.append(np.mean(layer_weights, axis=0))\n",
    "\n",
    "    avg_loss = np.mean(all_losses)\n",
    "    print(\"Average Loss:\")\n",
    "    print(avg_loss)\n",
    "    loss_of_all_models.append(avg_loss)\n",
    "\n",
    "    counter_loss = float('inf')\n",
    "    if avg_loss < counter_loss:\n",
    "        counter_loss = avg_loss\n",
    "    print(\"Counter Loss:\")\n",
    "    print(counter_loss)\n",
    "\n",
    "    flat_weights = [np.concatenate([layer.flatten() for layer in client_weights]) for client_weights in all_weights]\n",
    "    weighted_matrix = np.vstack(flat_weights)\n",
    "\n",
    "    clustering = AgglomerativeClustering(n_clusters=num_clusters).fit(weighted_matrix)\n",
    "    cluster_labels = clustering.labels_\n",
    "    print(\"Cluster Labels:\", cluster_labels)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_weights = pca.fit_transform(weighted_matrix)\n",
    "    plt.scatter(reduced_weights[:, 0], reduced_weights[:, 1], c=cluster_labels, cmap='viridis')\n",
    "    plt.title('Agglomerative Clustering')\n",
    "    plt.xlabel('Component 1')\n",
    "    plt.ylabel('Component 2')\n",
    "    plt.colorbar(label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "    return avg_weights, cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_initial_weights(shape):\n",
    "    initial_weights = [np.random.rand(*s) for s in shape]\n",
    "    return initial_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.1\n",
    "d = 1\n",
    "num_features = 71\n",
    "avg_weights = initialize_initial_weights([(70, 64), (64,), (64, 32), (32,), (32, 1), (1,)])\n",
    "num_blocks = 5000\n",
    "num_clients = 8\n",
    "p = num_clients  # Initialize the number of clusters to the number of clients\n",
    "loss_of_all_models = []\n",
    "\n",
    "for block in range(num_blocks):\n",
    "    all_weights = []\n",
    "    all_losses = []\n",
    "    for i, client_df in enumerate(client_dfs):\n",
    "        block_size = len(client_df) // num_blocks\n",
    "        start_idx = block * block_size\n",
    "        end_idx = (block + 1) * block_size\n",
    "        block_df = client_df.iloc[start_idx:end_idx]\n",
    "        X = block_df.drop(columns=['Label'])\n",
    "        y = block_df['Label']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        weights, loss = train_model_with_feedback(X_train, y_train, X_test, y_test, avg_weights)\n",
    "        all_weights.append(weights)\n",
    "        all_losses.append(loss)\n",
    "        print(f\"Block {block + 1}:\")\n",
    "        print(f\"Client {i + 1} Loss after block {block + 1}: {loss}\")\n",
    "\n",
    "    avg_weights, cluster_labels = server(all_weights, all_losses, p)\n",
    "\n",
    "    if block > 0:\n",
    "        reduction_ratio = (loss_of_all_models[-2] - loss_of_all_models[-1]) / loss_of_all_models[-2]\n",
    "        if reduction_ratio > w:\n",
    "            p -= d\n",
    "            d *= 2\n",
    "        else:\n",
    "            p = min(p * 2, num_clients)\n",
    "            d = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
